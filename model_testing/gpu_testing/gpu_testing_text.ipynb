{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2416e3-36ce-4fc1-a1a3-8ed77f918bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a1025-4947-4ed9-bdb1-4b4ca23eb947",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b933c4-2085-4340-a206-57d17e126eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and preprocess\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "val_dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea368e4-bb16-4c92-8574-7bddaded8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20ce72fce0145dfaa672e9e0b1a8dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization and DataLoader\n",
    "def preprocess_data(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": tokens[\"input_ids\"].squeeze(0), \"label\": torch.tensor(examples[\"label\"]).float()}\n",
    "\n",
    "tokenized_data = dataset.map(preprocess_data, batched=True)\n",
    "X_train = torch.stack([torch.tensor(x) for x in tokenized_data[\"input_ids\"]])\n",
    "y_train = torch.tensor(tokenized_data[\"label\"]).float()\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_tokenized_data = val_dataset.map(preprocess_data, batched=True)\n",
    "X_val = torch.stack([torch.tensor(x) for x in val_tokenized_data[\"input_ids\"]])\n",
    "y_val = torch.tensor(val_tokenized_data[\"label\"]).float()\n",
    "val_dataset = TensorDataset(X_train, y_train)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1dcd1c-888a-43b2-8e6d-3f9ef37cf631",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to train model with validation set\n",
    "def train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=5):\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0  # Accumulate training loss for averaging\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        # Training loop\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "        avg_train_loss = epoch_loss / total_batches  # Compute average training loss\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                y_val_pred = model(x_val)\n",
    "                val_loss += criterion(y_val_pred.squeeze(), y_val).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)  # Compute average validation loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a90d8-d078-4ede-bd69-7ed29fc71be0",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ec349c-07a4-432b-9f97-7f24856145a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model for Sentiment Analysis\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd469193-0f78-422e-8320-c4294a6de9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a5f4d7-1ead-4706-b4cb-3aadbf62ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6908, Val Loss: 0.6858\n",
      "Epoch 2/10, Train Loss: 0.6934, Val Loss: 0.6923\n",
      "Epoch 3/10, Train Loss: 0.6919, Val Loss: 0.6833\n",
      "Epoch 4/10, Train Loss: 0.6930, Val Loss: 0.6904\n",
      "Epoch 5/10, Train Loss: 0.6852, Val Loss: 0.6909\n",
      "Epoch 6/10, Train Loss: 0.6596, Val Loss: 0.6356\n",
      "Epoch 7/10, Train Loss: 0.5699, Val Loss: 0.4792\n",
      "Epoch 8/10, Train Loss: 0.3898, Val Loss: 0.2975\n",
      "Epoch 9/10, Train Loss: 0.2856, Val Loss: 0.2292\n",
      "Epoch 10/10, Train Loss: 0.2240, Val Loss: 0.1759\n",
      "Training Time on cpu: 835.93 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = SentimentLSTM(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3697287-7afb-414a-896b-192ad5527ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6926, Val Loss: 0.6903\n",
      "Epoch 2/10, Train Loss: 0.6850, Val Loss: 0.6752\n",
      "Epoch 3/10, Train Loss: 0.6890, Val Loss: 0.6905\n",
      "Epoch 4/10, Train Loss: 0.6904, Val Loss: 0.6905\n",
      "Epoch 5/10, Train Loss: 0.6891, Val Loss: 0.6880\n",
      "Epoch 6/10, Train Loss: 0.6699, Val Loss: 0.7007\n",
      "Epoch 7/10, Train Loss: 0.6792, Val Loss: 0.6171\n",
      "Epoch 8/10, Train Loss: 0.5824, Val Loss: 0.4945\n",
      "Epoch 9/10, Train Loss: 0.4830, Val Loss: 0.4456\n",
      "Epoch 10/10, Train Loss: 0.4073, Val Loss: 0.3580\n",
      "Training Time on mps: 80.59 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentLSTM(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560875c5-b55f-40ff-9f2d-3ed9d2f76aca",
   "metadata": {},
   "source": [
    "## BiLSTMAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "777b3382-9909-4343-887f-196159ee68db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentBiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(SentimentBiLSTMAttention, self).__init__()\n",
    "        # Embedding layer converts token indices to embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Bidirectional LSTM to capture context from both directions\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Attention layer: projects each hidden state into a single attention score\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # Final fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings\n",
    "        embedded = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Pass embeddings through the bidirectional LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, sequence_length, hidden_size*2)\n",
    "        \n",
    "        # Compute attention weights for each time step\n",
    "        # The attention layer produces a score for each hidden state\n",
    "        attn_scores = torch.tanh(self.attention(lstm_out))  # (batch_size, sequence_length, 1)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)          # (batch_size, sequence_length, 1)\n",
    "        \n",
    "        # Compute the context vector as the weighted sum of LSTM outputs\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)  # (batch_size, hidden_size*2)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Final classification output\n",
    "        out = self.fc(context_vector)  # (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16e4e61a-4bbe-42e2-aa7d-4e6f4b70dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5409, Val Loss: 0.4139\n",
      "Epoch 2/10, Train Loss: 0.3524, Val Loss: 0.2658\n",
      "Epoch 3/10, Train Loss: 0.2642, Val Loss: 0.1995\n",
      "Epoch 4/10, Train Loss: 0.2114, Val Loss: 0.1609\n",
      "Epoch 5/10, Train Loss: 0.1687, Val Loss: 0.1043\n",
      "Epoch 6/10, Train Loss: 0.1108, Val Loss: 0.0640\n",
      "Epoch 7/10, Train Loss: 0.0692, Val Loss: 0.1541\n",
      "Epoch 8/10, Train Loss: 0.0493, Val Loss: 0.0192\n",
      "Epoch 9/10, Train Loss: 0.0269, Val Loss: 0.0223\n",
      "Epoch 10/10, Train Loss: 0.0189, Val Loss: 0.0083\n",
      "Training Time on mps: 182.22 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentBiLSTMAttention(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a001ae4-1047-440b-b891-a8aefb3b6bd8",
   "metadata": {},
   "source": [
    "## SentimentAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13ef8f77-9af8-4031-8baa-a31ff1a85730",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        output_size, \n",
    "        dropout=0.5, \n",
    "        n_heads=4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Number of tokens in the vocabulary.\n",
    "            embedding_dim (int): Dimensionality of the word embeddings.\n",
    "            hidden_size (int): Hidden state size of the LSTM.\n",
    "            num_layers (int): Number of LSTM layers.\n",
    "            output_size (int): Number of output classes (e.g., sentiment labels).\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of attention heads for multi-head attention.\n",
    "        \"\"\"\n",
    "        super(SentimentAdvanced, self).__init__()\n",
    "        # Embedding layer converts token indices to embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Bidirectional LSTM to capture context from both directions.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Multi-head self-attention layer.\n",
    "        # Note: embed_dim for attention is hidden_size * 2 due to bidirectionality.\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2, \n",
    "            num_heads=n_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for stabilizing training.\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        \n",
    "        # Final fully connected layer for classification.\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        \n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, output_size) containing class scores.\n",
    "        \"\"\"\n",
    "        # 1. Embed the input tokens.\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # 2. Pass the embeddings through the bidirectional LSTM.\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # 3. Apply multi-head self-attention.\n",
    "        #    Using LSTM outputs as query, key, and value.\n",
    "        attn_out, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)\n",
    "        # 4. Add a residual connection and normalize.\n",
    "        attn_out = self.layer_norm(attn_out + lstm_out)\n",
    "        \n",
    "        # 5. Pool the sequence output (max pooling over the time dimension).\n",
    "        #    This extracts the most salient features across the sequence.\n",
    "        pooled, _ = torch.max(attn_out, dim=1)  # Shape: (batch_size, hidden_size*2)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        # 6. Final classification layer.\n",
    "        out = self.fc(pooled)  # Shape: (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acdbd921-99b4-40d4-8dd6-d7f4623dcde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6642, Val Loss: 0.4789\n",
      "Epoch 2/10, Train Loss: 0.3962, Val Loss: 0.2878\n",
      "Epoch 3/10, Train Loss: 0.2792, Val Loss: 0.2255\n",
      "Epoch 4/10, Train Loss: 0.2173, Val Loss: 0.1502\n",
      "Epoch 5/10, Train Loss: 0.1509, Val Loss: 0.0999\n",
      "Epoch 6/10, Train Loss: 0.1133, Val Loss: 0.0660\n",
      "Epoch 7/10, Train Loss: 0.0717, Val Loss: 0.0322\n",
      "Epoch 8/10, Train Loss: 0.0441, Val Loss: 0.0233\n",
      "Epoch 9/10, Train Loss: 0.0386, Val Loss: 0.0189\n",
      "Epoch 10/10, Train Loss: 0.0260, Val Loss: 0.0134\n",
      "Training Time on mps: 301.12 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader,val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c2ddd-884a-4460-b819-1204f2d92fd1",
   "metadata": {},
   "source": [
    "## SentimentUltraAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2c8597a-2559-4d5b-8cfa-a2946a10e9bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentUltraAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        lstm_layers,\n",
    "        transformer_layers,\n",
    "        num_filters,\n",
    "        output_size,\n",
    "        dropout=0.5,\n",
    "        n_heads=4,\n",
    "        cnn_kernel_sizes=[2, 3, 4]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of word embeddings.\n",
    "            hidden_size (int): Hidden state size for the LSTM.\n",
    "            lstm_layers (int): Number of LSTM layers.\n",
    "            transformer_layers (int): Number of Transformer encoder layers.\n",
    "            num_filters (int): Number of CNN filters per kernel size.\n",
    "            output_size (int): Number of output classes (e.g., sentiment categories).\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of heads in the Transformer encoder.\n",
    "            cnn_kernel_sizes (list): List of kernel sizes for the CNN branch.\n",
    "        \"\"\"\n",
    "        super(SentimentUltraAdvanced, self).__init__()\n",
    "        # Embedding layer: converts token indices to embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # CNN branch: a set of 1D convolutional layers with different kernel sizes.\n",
    "        self.cnn_convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=ks\n",
    "            )\n",
    "            for ks in cnn_kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # LSTM branch: Bidirectional LSTM to capture sequential context.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder: further refines LSTM outputs using multi-head self-attention.\n",
    "        # (Note: With PyTorch 1.9+ you can set batch_first=True in TransformerEncoderLayer.)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size * 2,  # Because LSTM is bidirectional.\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "        \n",
    "        # Compute the combined feature dimension.\n",
    "        # CNN branch: len(cnn_kernel_sizes) * num_filters.\n",
    "        # LSTM/Transformer branch: hidden_size * 2.\n",
    "        combined_dim = num_filters * len(cnn_kernel_sizes) + hidden_size * 2\n",
    "        \n",
    "        # Final fully connected layer for classification.\n",
    "        self.fc = nn.Linear(combined_dim, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # 2. CNN branch:\n",
    "        # Permute to (batch_size, embedding_dim, seq_length) for Conv1d.\n",
    "        cnn_input = embedded.permute(0, 2, 1)\n",
    "        cnn_features = []\n",
    "        for conv in self.cnn_convs:\n",
    "            # Apply convolution -> non-linearity -> global max pooling.\n",
    "            conv_out = conv(cnn_input)         # Shape: (batch, num_filters, L_out)\n",
    "            conv_out = F.relu(conv_out)\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.shape[2])  # Shape: (batch, num_filters, 1)\n",
    "            pooled = pooled.squeeze(2)         # Shape: (batch, num_filters)\n",
    "            cnn_features.append(pooled)\n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # Shape: (batch, num_filters * len(cnn_kernel_sizes))\n",
    "        \n",
    "        # 3. LSTM + Transformer branch:\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch, seq_length, hidden_size*2)\n",
    "        # Refine LSTM outputs with Transformer encoder.\n",
    "        transformer_out = self.transformer_encoder(lstm_out)  # Shape: (batch, seq_length, hidden_size*2)\n",
    "        # Global average pooling over time (sequence length) dimension.\n",
    "        transformer_features = torch.mean(transformer_out, dim=1)  # Shape: (batch, hidden_size*2)\n",
    "        \n",
    "        # 4. Feature Fusion:\n",
    "        combined = torch.cat([cnn_features, transformer_features], dim=1)  # Shape: (batch, combined_dim)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # 5. Final classification layer.\n",
    "        output = self.fc(combined)  # Shape: (batch, output_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae9a3d99-f69c-4925-a46f-a1d1ad1e8e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6737, Val Loss: 0.5184\n",
      "Epoch 2/10, Train Loss: 0.5403, Val Loss: 0.4314\n",
      "Epoch 3/10, Train Loss: 0.4798, Val Loss: 0.3699\n",
      "Epoch 4/10, Train Loss: 0.4338, Val Loss: 0.3645\n",
      "Epoch 5/10, Train Loss: 0.3950, Val Loss: 0.2966\n",
      "Epoch 6/10, Train Loss: 0.3657, Val Loss: 0.2665\n",
      "Epoch 7/10, Train Loss: 0.3365, Val Loss: 0.2310\n",
      "Epoch 8/10, Train Loss: 0.3051, Val Loss: 0.2172\n",
      "Epoch 9/10, Train Loss: 0.2783, Val Loss: 0.1764\n",
      "Epoch 10/10, Train Loss: 0.2526, Val Loss: 0.1512\n",
      "Training Time on mps: 807.49 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentUltraAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, lstm_layers=2, transformer_layers=2, num_filters=64, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b0bda-8356-49d5-a417-0f86782dca91",
   "metadata": {},
   "source": [
    "## SentimentUltraUltraAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a58dbc4e-c8b0-4a60-8277-9dd8cba9a090",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentUltraUltraAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        lstm_layers,\n",
    "        transformer_layers,\n",
    "        num_filters,\n",
    "        output_size,\n",
    "        dropout=0.5,\n",
    "        n_heads=4,\n",
    "        cnn_kernel_sizes=[2, 3, 4],\n",
    "        self_attn_layers=2,\n",
    "        max_seq_length=512  # adjust based on your expected maximum sequence length\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Vocabulary size.\n",
    "            embedding_dim (int): Dimension of word embeddings.\n",
    "            hidden_size (int): Hidden size for the LSTM.\n",
    "            lstm_layers (int): Number of LSTM layers.\n",
    "            transformer_layers (int): Number of Transformer encoder layers for the LSTM branch.\n",
    "            num_filters (int): Number of CNN filters per kernel size.\n",
    "            output_size (int): Number of output classes.\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of attention heads in Transformer encoders.\n",
    "            cnn_kernel_sizes (list): List of kernel sizes for the CNN branch.\n",
    "            self_attn_layers (int): Number of Transformer encoder layers in the self-attention branch.\n",
    "            max_seq_length (int): Maximum expected sequence length (for positional embeddings).\n",
    "        \"\"\"\n",
    "        super(SentimentUltraUltraAdvanced, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Learnable positional embeddings for the self-attention branch\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, embedding_dim))\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 1: CNN for local features\n",
    "        # ----------------------------\n",
    "        # Create one convolution per kernel size.\n",
    "        self.cnn_convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            ) for k in cnn_kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 2: LSTM + Transformer for sequential modeling\n",
    "        # ----------------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        # Transformer encoder refines the LSTM outputs.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size * 2,  # bidirectional\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm_transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 3: Direct Self-Attention on embeddings\n",
    "        # ----------------------------\n",
    "        encoder_layer2 = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.self_attn_encoder = nn.TransformerEncoder(encoder_layer2, num_layers=self_attn_layers)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Gating Mechanism to fuse branch features dynamically\n",
    "        # ----------------------------\n",
    "        # Compute dimensions for each branch's output:\n",
    "        cnn_out_dim = num_filters * len(cnn_kernel_sizes)\n",
    "        lstm_out_dim = hidden_size * 2  # from bidirectional LSTM\n",
    "        self_attn_out_dim = embedding_dim\n",
    "        \n",
    "        # Linear layers to compute a gate (a scalar weight) for each branch.\n",
    "        self.gate_cnn = nn.Linear(cnn_out_dim, 1)\n",
    "        self.gate_lstm = nn.Linear(lstm_out_dim, 1)\n",
    "        self.gate_self_attn = nn.Linear(self_attn_out_dim, 1)\n",
    "        \n",
    "        # Final fusion dimension is the concatenation of all branch outputs.\n",
    "        fused_dim = cnn_out_dim + lstm_out_dim + self_attn_out_dim\n",
    "        \n",
    "        # Final classification layer.\n",
    "        self.fc = nn.Linear(fused_dim, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, sequence_length) with token indices.\n",
    "        Returns:\n",
    "            Tensor: Logits of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.size()\n",
    "        # 1. Embedding lookup.\n",
    "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 1: CNN\n",
    "        # ----------------------------\n",
    "        # For Conv1d, we need shape: (batch_size, embedding_dim, seq_length)\n",
    "        cnn_input = embedded.permute(0, 2, 1)\n",
    "        cnn_features = []\n",
    "        for conv in self.cnn_convs:\n",
    "            conv_out = F.relu(conv(cnn_input))  # (batch, num_filters, L_out)\n",
    "            # Global max pooling over the temporal (L_out) dimension.\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.size(2)).squeeze(2)  # (batch, num_filters)\n",
    "            cnn_features.append(pooled)\n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # (batch, num_filters * len(cnn_kernel_sizes))\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 2: LSTM + Transformer\n",
    "        # ----------------------------\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch, seq_length, hidden_size*2)\n",
    "        # Refine with Transformer encoder.\n",
    "        lstm_transformed = self.lstm_transformer(lstm_out)  # (batch, seq_length, hidden_size*2)\n",
    "        # Global average pooling over the time dimension.\n",
    "        lstm_features = torch.mean(lstm_transformed, dim=1)  # (batch, hidden_size*2)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 3: Direct Self-Attention on embeddings\n",
    "        # ----------------------------\n",
    "        # Add positional embeddings (truncate or expand to the current sequence length)\n",
    "        pos_emb = self.pos_embedding[:, :seq_length, :]  # (1, seq_length, embedding_dim)\n",
    "        self_attn_input = embedded + pos_emb\n",
    "        self_attn_out = self.self_attn_encoder(self_attn_input)  # (batch, seq_length, embedding_dim)\n",
    "        # Global max pooling over the time dimension.\n",
    "        self_attn_features, _ = torch.max(self_attn_out, dim=1)  # (batch, embedding_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Gating: Compute dynamic weights for each branch.\n",
    "        # ----------------------------\n",
    "        gate_cnn = torch.sigmoid(self.gate_cnn(cnn_features))           # (batch, 1)\n",
    "        gate_lstm = torch.sigmoid(self.gate_lstm(lstm_features))          # (batch, 1)\n",
    "        gate_self_attn = torch.sigmoid(self.gate_self_attn(self_attn_features))  # (batch, 1)\n",
    "        \n",
    "        gated_cnn = cnn_features * gate_cnn         # (batch, cnn_out_dim)\n",
    "        gated_lstm = lstm_features * gate_lstm        # (batch, lstm_out_dim)\n",
    "        gated_self_attn = self_attn_features * gate_self_attn  # (batch, self_attn_out_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Feature Fusion and Classification\n",
    "        # ----------------------------\n",
    "        # Concatenate the gated features from all branches.\n",
    "        combined = torch.cat([gated_cnn, gated_lstm, gated_self_attn], dim=1)  # (batch, fused_dim)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.fc(combined)  # (batch, output_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3e43649-a93d-4163-99ae-d11c0640d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6241, Val Loss: 0.4785\n",
      "Epoch 2/10, Train Loss: 0.3864, Val Loss: 0.7370\n",
      "Epoch 3/10, Train Loss: 0.2459, Val Loss: 0.3226\n",
      "Epoch 4/10, Train Loss: 0.1488, Val Loss: 0.1486\n",
      "Epoch 5/10, Train Loss: 0.0780, Val Loss: 0.0310\n",
      "Epoch 6/10, Train Loss: 0.0343, Val Loss: 0.0087\n",
      "Epoch 7/10, Train Loss: 0.0195, Val Loss: 0.0088\n",
      "Epoch 8/10, Train Loss: 0.0124, Val Loss: 0.0055\n",
      "Epoch 9/10, Train Loss: 0.0076, Val Loss: 0.0030\n",
      "Epoch 10/10, Train Loss: 0.0072, Val Loss: 0.0026\n",
      "Training Time on mps: 1153.60 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentUltraUltraAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, lstm_layers=2, transformer_layers=2, num_filters=64, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader,val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ff339-7d8b-44a7-9c0f-c07991d6b38f",
   "metadata": {},
   "source": [
    "# Amazon Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923bb172-137f-4a31-a1d6-a2fd5a6b36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a74a558-23fc-44d2-ba65-7a45bd7a2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess the data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Use the Amazon Polarity dataset from Hugging Face.\n",
    "# We use the provided 'train' split for training and 'test' as our validation set.\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "val_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # Tokenize the text; we use padding and truncation for consistency.\n",
    "    # tokens = tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Remove extra batch dimension from tokens; create a tensor for the label.\n",
    "    return {\"input_ids\": tokens[\"input_ids\"].squeeze(0), \"label\": torch.tensor(examples[\"label\"]).float()}\n",
    "\n",
    "# Apply preprocessing to both training and validation datasets\n",
    "train_tokenized = train_dataset.map(preprocess_data, batched=True)\n",
    "val_tokenized = val_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Convert lists of input_ids and labels into tensors and create TensorDatasets.\n",
    "# (Note: The tokenizer returns lists, so we use torch.stack to combine them.)\n",
    "X_train = torch.stack([torch.tensor(x) for x in train_tokenized[\"input_ids\"]])\n",
    "y_train = torch.tensor(train_tokenized[\"label\"]).float()\n",
    "train_tensor_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "X_val = torch.stack([torch.tensor(x) for x in val_tokenized[\"input_ids\"]])\n",
    "y_val = torch.tensor(val_tokenized[\"label\"]).float()\n",
    "val_tensor_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders for training and validation.\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor_dataset, batch_size=batch_size)\n",
    "\n",
    "# Get vocabulary size from the tokenizer (BERT-base-uncased has vocab size 30522)\n",
    "VOCAB_SIZE = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083b0524-5902-42c2-b9ea-97255cd0b333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define the Sentiment BiLSTM with Attention Model\n",
    "class SentimentBiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(SentimentBiLSTMAttention, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_scores = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        out = self.fc(context_vector)\n",
    "        return out\n",
    "\n",
    "# 3. Implement Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def check(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            return False  # Continue training\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Early stopping triggered after {self.counter} epochs.\")\n",
    "                return True  # Stop training\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d5da20-debe-4116-92cd-d401f9edcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function with Checkpointing\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs):\n",
    "    early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Apply Gradient Clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(outputs)).detach()\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.round(torch.sigmoid(outputs))\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        scheduler.step(val_loss)  # Reduce LR if validation loss stagnates\n",
    "\n",
    "        # Save Best Model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "        # Dynamic Dropout Adjustment\n",
    "        if val_loss > best_val_loss:\n",
    "            model.dropout.p = min(model.dropout.p + 0.05, 0.7)\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if early_stopping.check(val_loss):\n",
    "            break  # Stop training if no improvement\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Load Best Model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53aab302-9f40-4679-9165-cc4c58171665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Improved Hyperparameters\n",
    "embedding_dim = 256  # Remains unchanged\n",
    "hidden_size = 128  # Reduced from 256 to 128\n",
    "num_layers = 2  # Reduced from 3 to 2\n",
    "output_size = 1  # Binary classification\n",
    "dropout = 0.5  # Increased from 0.3 to 0.5 for better regularization\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = SentimentBiLSTMAttention(VOCAB_SIZE, embedding_dim, hidden_size, num_layers, output_size, dropout).to(device)\n",
    "\n",
    "# 4. Define Loss, Optimizer, and Learning Rate Scheduler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63126992-8b7d-4fa5-bc91-bf8a7690489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 0.5541, Train Acc: 0.6879, Val Loss: 0.3979, Val Acc: 0.8190\n",
      "Epoch 2/10: Train Loss: 0.3146, Train Acc: 0.8664, Val Loss: 0.3143, Val Acc: 0.8670\n",
      "Epoch 3/10: Train Loss: 0.2385, Train Acc: 0.9057, Val Loss: 0.3232, Val Acc: 0.8612\n",
      "Epoch 4/10: Train Loss: 0.1773, Train Acc: 0.9350, Val Loss: 0.3446, Val Acc: 0.8698\n",
      "Early stopping triggered after 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "# train_loss, train_acc, val_loss, val_acc = \n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e194ebf0-749e-4e70-8ece-56cd7e126f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Plot both accuracy and loss\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# # Plot accuracy\n",
    "# ax1.plot(range(1, epochs+1), train_acc, label=\"Train Accuracy\")\n",
    "# ax1.plot(range(1, epochs+1), val_acc, label=\"Validation Accuracy\")\n",
    "# ax1.set_xlabel(\"Epoch\")\n",
    "# ax1.set_ylabel(\"Accuracy\")\n",
    "# ax1.set_title(\"Accuracy Over Epochs\")\n",
    "# ax1.legend()\n",
    "# ax1.grid(True)\n",
    "\n",
    "# # Plot loss\n",
    "# ax2.plot(range(1, epochs+1), train_loss, label=\"Train Loss\")\n",
    "# ax2.plot(range(1, epochs+1), val_loss, label=\"Validation Loss\")\n",
    "# ax2.set_xlabel(\"Epoch\")\n",
    "# ax2.set_ylabel(\"Loss\")\n",
    "# ax2.set_title(\"Loss Over Epochs\")\n",
    "# ax2.legend()\n",
    "# ax2.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f876816-ea15-4412-aaad-2cfb07533e88",
   "metadata": {},
   "source": [
    "# CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d737f69-52ad-42a4-8051-43dd01e339b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d6906b-6bba-4d7b-9a86-7cdedf46facf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Define data transformations for training and testing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # Standard normalization for CIFAR-10\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# Split the training set into training and validation subsets (e.g., 90% training, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_subset, val_subset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load CIFAR-10 test data\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93be73ad-6012-4584-9e76-c93b5bbe4672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- Squeeze-and-Excitation Block (same as before) ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        squeeze = self.global_pool(x).view(b, c)\n",
    "        excitation = torch.sigmoid(self.fc2(F.relu(self.fc1(squeeze))))\n",
    "        excitation = excitation.view(b, c, 1, 1)\n",
    "        return x * excitation\n",
    "\n",
    "# --- ResNeXt Block with SE ---\n",
    "class ResNeXtBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cardinality=32, stride=1, downsample=False):\n",
    "        super(ResNeXtBlock, self).__init__()\n",
    "        # Bottleneck channels: typically a reduction factor is applied\n",
    "        mid_channels = out_channels // 2\n",
    "        \n",
    "        # 1x1 convolution for dimension reduction\n",
    "        self.conv_reduce = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)\n",
    "        self.bn_reduce = nn.BatchNorm2d(mid_channels)\n",
    "        \n",
    "        # 3x3 grouped convolution: using the specified cardinality (number of groups)\n",
    "        self.conv_conv = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride,\n",
    "                                   padding=1, groups=cardinality, bias=False)\n",
    "        self.bn_conv = nn.BatchNorm2d(mid_channels)\n",
    "        \n",
    "        # 1x1 convolution for dimension restoration\n",
    "        self.conv_expand = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn_expand = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # SE module to recalibrate features\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        # Shortcut connection in case of dimension mismatch or stride > 1\n",
    "        self.downsample = None\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv_reduce(x)\n",
    "        out = self.bn_reduce(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.conv_conv(out)\n",
    "        out = self.bn_conv(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.conv_expand(out)\n",
    "        out = self.bn_expand(out)\n",
    "        \n",
    "        # Apply SE attention\n",
    "        out = self.se(out)\n",
    "        \n",
    "        # Adjust shortcut if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "# --- Advanced ResNeXt-SE Model for CIFAR-10 ---\n",
    "class ResNeXtSE_CIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10, cardinality=32):\n",
    "        super(ResNeXtSE_CIFAR10, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # Initial convolution: for 32x32 RGB images\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Create layers with increasing feature dimensions.\n",
    "        # You can adjust the number of blocks per layer to balance depth and computation.\n",
    "        self.layer1 = self._make_layer(64, num_blocks=3, cardinality=cardinality, stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks=4, cardinality=cardinality, stride=2)\n",
    "        self.layer3 = self._make_layer(256, num_blocks=6, cardinality=cardinality, stride=2)\n",
    "        self.layer4 = self._make_layer(512, num_blocks=3, cardinality=cardinality, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, out_channels, num_blocks, cardinality, stride):\n",
    "        layers = []\n",
    "        # First block may downsample and increase dimensions\n",
    "        layers.append(ResNeXtBlock(self.in_channels, out_channels, cardinality=cardinality,\n",
    "                                   stride=stride, downsample=True))\n",
    "        self.in_channels = out_channels\n",
    "        # The remaining blocks maintain dimensions\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResNeXtBlock(out_channels, out_channels, cardinality=cardinality, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# --- Early Stopping Implementation ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# --- Training Loop with Early Stopping ---\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, n_epochs=50, patience=10, device='cpu'):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valid_acc = correct / len(valid_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
    "        \n",
    "        # Check early stopping criteria\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e185546-d0fb-4830-893d-323d21550405",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNeXtSE_CIFAR10(num_classes=10, cardinality=32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6658e02d-61c9-4bfb-aa65-234563589bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.5732, Valid Loss: 1.3181, Valid Acc: 0.5210\n",
      "Validation loss decreased (inf --> 1.318115). Saving model...\n",
      "Epoch 2, Train Loss: 1.1814, Valid Loss: 1.1604, Valid Acc: 0.5862\n",
      "Validation loss decreased (1.318115 --> 1.160378). Saving model...\n",
      "Epoch 3, Train Loss: 0.9707, Valid Loss: 0.9100, Valid Acc: 0.6804\n",
      "Validation loss decreased (1.160378 --> 0.910014). Saving model...\n",
      "Epoch 4, Train Loss: 0.7931, Valid Loss: 0.7751, Valid Acc: 0.7196\n",
      "Validation loss decreased (0.910014 --> 0.775052). Saving model...\n",
      "Epoch 5, Train Loss: 0.7093, Valid Loss: 0.7311, Valid Acc: 0.7388\n",
      "Validation loss decreased (0.775052 --> 0.731124). Saving model...\n",
      "Epoch 6, Train Loss: 0.6521, Valid Loss: 0.6698, Valid Acc: 0.7620\n",
      "Validation loss decreased (0.731124 --> 0.669793). Saving model...\n",
      "Epoch 7, Train Loss: 0.5663, Valid Loss: 0.6206, Valid Acc: 0.7830\n",
      "Validation loss decreased (0.669793 --> 0.620552). Saving model...\n",
      "Epoch 8, Train Loss: 0.5330, Valid Loss: 0.5892, Valid Acc: 0.7910\n",
      "Validation loss decreased (0.620552 --> 0.589247). Saving model...\n",
      "Epoch 9, Train Loss: 0.5127, Valid Loss: 0.5813, Valid Acc: 0.7994\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 10, Train Loss: 0.4706, Valid Loss: 0.5599, Valid Acc: 0.8012\n",
      "Validation loss decreased (0.589247 --> 0.559861). Saving model...\n",
      "Epoch 11, Train Loss: 0.4509, Valid Loss: 0.5377, Valid Acc: 0.8120\n",
      "Validation loss decreased (0.559861 --> 0.537738). Saving model...\n",
      "Epoch 12, Train Loss: 0.4427, Valid Loss: 0.5436, Valid Acc: 0.8066\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 13, Train Loss: 0.4232, Valid Loss: 0.5139, Valid Acc: 0.8180\n",
      "Validation loss decreased (0.537738 --> 0.513921). Saving model...\n",
      "Epoch 14, Train Loss: 0.4091, Valid Loss: 0.5271, Valid Acc: 0.8212\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 15, Train Loss: 0.4051, Valid Loss: 0.5309, Valid Acc: 0.8172\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 16, Train Loss: 0.3979, Valid Loss: 0.5093, Valid Acc: 0.8214\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 17, Train Loss: 0.3923, Valid Loss: 0.5165, Valid Acc: 0.8212\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 18, Train Loss: 0.3880, Valid Loss: 0.5060, Valid Acc: 0.8270\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 19, Train Loss: 0.3826, Valid Loss: 0.5097, Valid Acc: 0.8270\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 20, Train Loss: 0.3799, Valid Loss: 0.5019, Valid Acc: 0.8260\n",
      "Validation loss decreased (0.513921 --> 0.501948). Saving model...\n",
      "Epoch 21, Train Loss: 0.3818, Valid Loss: 0.5124, Valid Acc: 0.8252\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 22, Train Loss: 0.3756, Valid Loss: 0.5105, Valid Acc: 0.8206\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 23, Train Loss: 0.3748, Valid Loss: 0.5114, Valid Acc: 0.8230\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 24, Train Loss: 0.3768, Valid Loss: 0.4998, Valid Acc: 0.8270\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 25, Train Loss: 0.3714, Valid Loss: 0.5148, Valid Acc: 0.8238\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 26, Train Loss: 0.3714, Valid Loss: 0.4994, Valid Acc: 0.8322\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 27, Train Loss: 0.3712, Valid Loss: 0.5203, Valid Acc: 0.8182\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 28, Train Loss: 0.3719, Valid Loss: 0.5062, Valid Acc: 0.8302\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 29, Train Loss: 0.3721, Valid Loss: 0.4999, Valid Acc: 0.8320\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 30, Train Loss: 0.3720, Valid Loss: 0.5036, Valid Acc: 0.8192\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, trainloader, valloader, criterion, optimizer, scheduler, n_epochs=50, patience=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468e0a01-613c-4e62-8bde-bf52f022c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/5p2bs93x2xgc8trrh56v3dpc0000gn/T/ipykernel_61814/1084059354.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('checkpoint.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.93%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        # Get predictions by finding the class with the maximum logit\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
