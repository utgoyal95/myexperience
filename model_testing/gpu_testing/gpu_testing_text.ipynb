{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2416e3-36ce-4fc1-a1a3-8ed77f918bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a1025-4947-4ed9-bdb1-4b4ca23eb947",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b933c4-2085-4340-a206-57d17e126eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and preprocess\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "val_dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea368e4-bb16-4c92-8574-7bddaded8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20ce72fce0145dfaa672e9e0b1a8dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization and DataLoader\n",
    "def preprocess_data(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": tokens[\"input_ids\"].squeeze(0), \"label\": torch.tensor(examples[\"label\"]).float()}\n",
    "\n",
    "tokenized_data = dataset.map(preprocess_data, batched=True)\n",
    "X_train = torch.stack([torch.tensor(x) for x in tokenized_data[\"input_ids\"]])\n",
    "y_train = torch.tensor(tokenized_data[\"label\"]).float()\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_tokenized_data = val_dataset.map(preprocess_data, batched=True)\n",
    "X_val = torch.stack([torch.tensor(x) for x in val_tokenized_data[\"input_ids\"]])\n",
    "y_val = torch.tensor(val_tokenized_data[\"label\"]).float()\n",
    "val_dataset = TensorDataset(X_train, y_train)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1dcd1c-888a-43b2-8e6d-3f9ef37cf631",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to train model with validation set\n",
    "def train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=5):\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0  # Accumulate training loss for averaging\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        # Training loop\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "        avg_train_loss = epoch_loss / total_batches  # Compute average training loss\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                y_val_pred = model(x_val)\n",
    "                val_loss += criterion(y_val_pred.squeeze(), y_val).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)  # Compute average validation loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a90d8-d078-4ede-bd69-7ed29fc71be0",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ec349c-07a4-432b-9f97-7f24856145a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model for Sentiment Analysis\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd469193-0f78-422e-8320-c4294a6de9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a5f4d7-1ead-4706-b4cb-3aadbf62ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6908, Val Loss: 0.6858\n",
      "Epoch 2/10, Train Loss: 0.6934, Val Loss: 0.6923\n",
      "Epoch 3/10, Train Loss: 0.6919, Val Loss: 0.6833\n",
      "Epoch 4/10, Train Loss: 0.6930, Val Loss: 0.6904\n",
      "Epoch 5/10, Train Loss: 0.6852, Val Loss: 0.6909\n",
      "Epoch 6/10, Train Loss: 0.6596, Val Loss: 0.6356\n",
      "Epoch 7/10, Train Loss: 0.5699, Val Loss: 0.4792\n",
      "Epoch 8/10, Train Loss: 0.3898, Val Loss: 0.2975\n",
      "Epoch 9/10, Train Loss: 0.2856, Val Loss: 0.2292\n",
      "Epoch 10/10, Train Loss: 0.2240, Val Loss: 0.1759\n",
      "Training Time on cpu: 835.93 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = SentimentLSTM(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3697287-7afb-414a-896b-192ad5527ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6926, Val Loss: 0.6903\n",
      "Epoch 2/10, Train Loss: 0.6850, Val Loss: 0.6752\n",
      "Epoch 3/10, Train Loss: 0.6890, Val Loss: 0.6905\n",
      "Epoch 4/10, Train Loss: 0.6904, Val Loss: 0.6905\n",
      "Epoch 5/10, Train Loss: 0.6891, Val Loss: 0.6880\n",
      "Epoch 6/10, Train Loss: 0.6699, Val Loss: 0.7007\n",
      "Epoch 7/10, Train Loss: 0.6792, Val Loss: 0.6171\n",
      "Epoch 8/10, Train Loss: 0.5824, Val Loss: 0.4945\n",
      "Epoch 9/10, Train Loss: 0.4830, Val Loss: 0.4456\n",
      "Epoch 10/10, Train Loss: 0.4073, Val Loss: 0.3580\n",
      "Training Time on mps: 80.59 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentLSTM(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560875c5-b55f-40ff-9f2d-3ed9d2f76aca",
   "metadata": {},
   "source": [
    "## BiLSTMAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "777b3382-9909-4343-887f-196159ee68db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentBiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(SentimentBiLSTMAttention, self).__init__()\n",
    "        # Embedding layer converts token indices to embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Bidirectional LSTM to capture context from both directions\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Attention layer: projects each hidden state into a single attention score\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # Final fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings\n",
    "        embedded = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Pass embeddings through the bidirectional LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, sequence_length, hidden_size*2)\n",
    "        \n",
    "        # Compute attention weights for each time step\n",
    "        # The attention layer produces a score for each hidden state\n",
    "        attn_scores = torch.tanh(self.attention(lstm_out))  # (batch_size, sequence_length, 1)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)          # (batch_size, sequence_length, 1)\n",
    "        \n",
    "        # Compute the context vector as the weighted sum of LSTM outputs\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)  # (batch_size, hidden_size*2)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Final classification output\n",
    "        out = self.fc(context_vector)  # (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16e4e61a-4bbe-42e2-aa7d-4e6f4b70dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5409, Val Loss: 0.4139\n",
      "Epoch 2/10, Train Loss: 0.3524, Val Loss: 0.2658\n",
      "Epoch 3/10, Train Loss: 0.2642, Val Loss: 0.1995\n",
      "Epoch 4/10, Train Loss: 0.2114, Val Loss: 0.1609\n",
      "Epoch 5/10, Train Loss: 0.1687, Val Loss: 0.1043\n",
      "Epoch 6/10, Train Loss: 0.1108, Val Loss: 0.0640\n",
      "Epoch 7/10, Train Loss: 0.0692, Val Loss: 0.1541\n",
      "Epoch 8/10, Train Loss: 0.0493, Val Loss: 0.0192\n",
      "Epoch 9/10, Train Loss: 0.0269, Val Loss: 0.0223\n",
      "Epoch 10/10, Train Loss: 0.0189, Val Loss: 0.0083\n",
      "Training Time on mps: 182.22 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentBiLSTMAttention(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a001ae4-1047-440b-b891-a8aefb3b6bd8",
   "metadata": {},
   "source": [
    "## SentimentAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13ef8f77-9af8-4031-8baa-a31ff1a85730",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        output_size, \n",
    "        dropout=0.5, \n",
    "        n_heads=4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Number of tokens in the vocabulary.\n",
    "            embedding_dim (int): Dimensionality of the word embeddings.\n",
    "            hidden_size (int): Hidden state size of the LSTM.\n",
    "            num_layers (int): Number of LSTM layers.\n",
    "            output_size (int): Number of output classes (e.g., sentiment labels).\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of attention heads for multi-head attention.\n",
    "        \"\"\"\n",
    "        super(SentimentAdvanced, self).__init__()\n",
    "        # Embedding layer converts token indices to embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Bidirectional LSTM to capture context from both directions.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Multi-head self-attention layer.\n",
    "        # Note: embed_dim for attention is hidden_size * 2 due to bidirectionality.\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2, \n",
    "            num_heads=n_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for stabilizing training.\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        \n",
    "        # Final fully connected layer for classification.\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        \n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, output_size) containing class scores.\n",
    "        \"\"\"\n",
    "        # 1. Embed the input tokens.\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # 2. Pass the embeddings through the bidirectional LSTM.\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # 3. Apply multi-head self-attention.\n",
    "        #    Using LSTM outputs as query, key, and value.\n",
    "        attn_out, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)\n",
    "        # 4. Add a residual connection and normalize.\n",
    "        attn_out = self.layer_norm(attn_out + lstm_out)\n",
    "        \n",
    "        # 5. Pool the sequence output (max pooling over the time dimension).\n",
    "        #    This extracts the most salient features across the sequence.\n",
    "        pooled, _ = torch.max(attn_out, dim=1)  # Shape: (batch_size, hidden_size*2)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        # 6. Final classification layer.\n",
    "        out = self.fc(pooled)  # Shape: (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acdbd921-99b4-40d4-8dd6-d7f4623dcde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6642, Val Loss: 0.4789\n",
      "Epoch 2/10, Train Loss: 0.3962, Val Loss: 0.2878\n",
      "Epoch 3/10, Train Loss: 0.2792, Val Loss: 0.2255\n",
      "Epoch 4/10, Train Loss: 0.2173, Val Loss: 0.1502\n",
      "Epoch 5/10, Train Loss: 0.1509, Val Loss: 0.0999\n",
      "Epoch 6/10, Train Loss: 0.1133, Val Loss: 0.0660\n",
      "Epoch 7/10, Train Loss: 0.0717, Val Loss: 0.0322\n",
      "Epoch 8/10, Train Loss: 0.0441, Val Loss: 0.0233\n",
      "Epoch 9/10, Train Loss: 0.0386, Val Loss: 0.0189\n",
      "Epoch 10/10, Train Loss: 0.0260, Val Loss: 0.0134\n",
      "Training Time on mps: 301.12 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader,val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c2ddd-884a-4460-b819-1204f2d92fd1",
   "metadata": {},
   "source": [
    "## SentimentUltraAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2c8597a-2559-4d5b-8cfa-a2946a10e9bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentUltraAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        lstm_layers,\n",
    "        transformer_layers,\n",
    "        num_filters,\n",
    "        output_size,\n",
    "        dropout=0.5,\n",
    "        n_heads=4,\n",
    "        cnn_kernel_sizes=[2, 3, 4]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of word embeddings.\n",
    "            hidden_size (int): Hidden state size for the LSTM.\n",
    "            lstm_layers (int): Number of LSTM layers.\n",
    "            transformer_layers (int): Number of Transformer encoder layers.\n",
    "            num_filters (int): Number of CNN filters per kernel size.\n",
    "            output_size (int): Number of output classes (e.g., sentiment categories).\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of heads in the Transformer encoder.\n",
    "            cnn_kernel_sizes (list): List of kernel sizes for the CNN branch.\n",
    "        \"\"\"\n",
    "        super(SentimentUltraAdvanced, self).__init__()\n",
    "        # Embedding layer: converts token indices to embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # CNN branch: a set of 1D convolutional layers with different kernel sizes.\n",
    "        self.cnn_convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=ks\n",
    "            )\n",
    "            for ks in cnn_kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # LSTM branch: Bidirectional LSTM to capture sequential context.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder: further refines LSTM outputs using multi-head self-attention.\n",
    "        # (Note: With PyTorch 1.9+ you can set batch_first=True in TransformerEncoderLayer.)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size * 2,  # Because LSTM is bidirectional.\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "        \n",
    "        # Compute the combined feature dimension.\n",
    "        # CNN branch: len(cnn_kernel_sizes) * num_filters.\n",
    "        # LSTM/Transformer branch: hidden_size * 2.\n",
    "        combined_dim = num_filters * len(cnn_kernel_sizes) + hidden_size * 2\n",
    "        \n",
    "        # Final fully connected layer for classification.\n",
    "        self.fc = nn.Linear(combined_dim, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # 2. CNN branch:\n",
    "        # Permute to (batch_size, embedding_dim, seq_length) for Conv1d.\n",
    "        cnn_input = embedded.permute(0, 2, 1)\n",
    "        cnn_features = []\n",
    "        for conv in self.cnn_convs:\n",
    "            # Apply convolution -> non-linearity -> global max pooling.\n",
    "            conv_out = conv(cnn_input)         # Shape: (batch, num_filters, L_out)\n",
    "            conv_out = F.relu(conv_out)\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.shape[2])  # Shape: (batch, num_filters, 1)\n",
    "            pooled = pooled.squeeze(2)         # Shape: (batch, num_filters)\n",
    "            cnn_features.append(pooled)\n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # Shape: (batch, num_filters * len(cnn_kernel_sizes))\n",
    "        \n",
    "        # 3. LSTM + Transformer branch:\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch, seq_length, hidden_size*2)\n",
    "        # Refine LSTM outputs with Transformer encoder.\n",
    "        transformer_out = self.transformer_encoder(lstm_out)  # Shape: (batch, seq_length, hidden_size*2)\n",
    "        # Global average pooling over time (sequence length) dimension.\n",
    "        transformer_features = torch.mean(transformer_out, dim=1)  # Shape: (batch, hidden_size*2)\n",
    "        \n",
    "        # 4. Feature Fusion:\n",
    "        combined = torch.cat([cnn_features, transformer_features], dim=1)  # Shape: (batch, combined_dim)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # 5. Final classification layer.\n",
    "        output = self.fc(combined)  # Shape: (batch, output_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae9a3d99-f69c-4925-a46f-a1d1ad1e8e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6737, Val Loss: 0.5184\n",
      "Epoch 2/10, Train Loss: 0.5403, Val Loss: 0.4314\n",
      "Epoch 3/10, Train Loss: 0.4798, Val Loss: 0.3699\n",
      "Epoch 4/10, Train Loss: 0.4338, Val Loss: 0.3645\n",
      "Epoch 5/10, Train Loss: 0.3950, Val Loss: 0.2966\n",
      "Epoch 6/10, Train Loss: 0.3657, Val Loss: 0.2665\n",
      "Epoch 7/10, Train Loss: 0.3365, Val Loss: 0.2310\n",
      "Epoch 8/10, Train Loss: 0.3051, Val Loss: 0.2172\n",
      "Epoch 9/10, Train Loss: 0.2783, Val Loss: 0.1764\n",
      "Epoch 10/10, Train Loss: 0.2526, Val Loss: 0.1512\n",
      "Training Time on mps: 807.49 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentUltraAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, lstm_layers=2, transformer_layers=2, num_filters=64, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b0bda-8356-49d5-a417-0f86782dca91",
   "metadata": {},
   "source": [
    "## SentimentUltraUltraAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a58dbc4e-c8b0-4a60-8277-9dd8cba9a090",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentUltraUltraAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        lstm_layers,\n",
    "        transformer_layers,\n",
    "        num_filters,\n",
    "        output_size,\n",
    "        dropout=0.5,\n",
    "        n_heads=4,\n",
    "        cnn_kernel_sizes=[2, 3, 4],\n",
    "        self_attn_layers=2,\n",
    "        max_seq_length=512  # adjust based on your expected maximum sequence length\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Vocabulary size.\n",
    "            embedding_dim (int): Dimension of word embeddings.\n",
    "            hidden_size (int): Hidden size for the LSTM.\n",
    "            lstm_layers (int): Number of LSTM layers.\n",
    "            transformer_layers (int): Number of Transformer encoder layers for the LSTM branch.\n",
    "            num_filters (int): Number of CNN filters per kernel size.\n",
    "            output_size (int): Number of output classes.\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of attention heads in Transformer encoders.\n",
    "            cnn_kernel_sizes (list): List of kernel sizes for the CNN branch.\n",
    "            self_attn_layers (int): Number of Transformer encoder layers in the self-attention branch.\n",
    "            max_seq_length (int): Maximum expected sequence length (for positional embeddings).\n",
    "        \"\"\"\n",
    "        super(SentimentUltraUltraAdvanced, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Learnable positional embeddings for the self-attention branch\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, embedding_dim))\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 1: CNN for local features\n",
    "        # ----------------------------\n",
    "        # Create one convolution per kernel size.\n",
    "        self.cnn_convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            ) for k in cnn_kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 2: LSTM + Transformer for sequential modeling\n",
    "        # ----------------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        # Transformer encoder refines the LSTM outputs.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size * 2,  # bidirectional\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm_transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 3: Direct Self-Attention on embeddings\n",
    "        # ----------------------------\n",
    "        encoder_layer2 = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.self_attn_encoder = nn.TransformerEncoder(encoder_layer2, num_layers=self_attn_layers)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Gating Mechanism to fuse branch features dynamically\n",
    "        # ----------------------------\n",
    "        # Compute dimensions for each branch's output:\n",
    "        cnn_out_dim = num_filters * len(cnn_kernel_sizes)\n",
    "        lstm_out_dim = hidden_size * 2  # from bidirectional LSTM\n",
    "        self_attn_out_dim = embedding_dim\n",
    "        \n",
    "        # Linear layers to compute a gate (a scalar weight) for each branch.\n",
    "        self.gate_cnn = nn.Linear(cnn_out_dim, 1)\n",
    "        self.gate_lstm = nn.Linear(lstm_out_dim, 1)\n",
    "        self.gate_self_attn = nn.Linear(self_attn_out_dim, 1)\n",
    "        \n",
    "        # Final fusion dimension is the concatenation of all branch outputs.\n",
    "        fused_dim = cnn_out_dim + lstm_out_dim + self_attn_out_dim\n",
    "        \n",
    "        # Final classification layer.\n",
    "        self.fc = nn.Linear(fused_dim, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, sequence_length) with token indices.\n",
    "        Returns:\n",
    "            Tensor: Logits of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.size()\n",
    "        # 1. Embedding lookup.\n",
    "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 1: CNN\n",
    "        # ----------------------------\n",
    "        # For Conv1d, we need shape: (batch_size, embedding_dim, seq_length)\n",
    "        cnn_input = embedded.permute(0, 2, 1)\n",
    "        cnn_features = []\n",
    "        for conv in self.cnn_convs:\n",
    "            conv_out = F.relu(conv(cnn_input))  # (batch, num_filters, L_out)\n",
    "            # Global max pooling over the temporal (L_out) dimension.\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.size(2)).squeeze(2)  # (batch, num_filters)\n",
    "            cnn_features.append(pooled)\n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # (batch, num_filters * len(cnn_kernel_sizes))\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 2: LSTM + Transformer\n",
    "        # ----------------------------\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch, seq_length, hidden_size*2)\n",
    "        # Refine with Transformer encoder.\n",
    "        lstm_transformed = self.lstm_transformer(lstm_out)  # (batch, seq_length, hidden_size*2)\n",
    "        # Global average pooling over the time dimension.\n",
    "        lstm_features = torch.mean(lstm_transformed, dim=1)  # (batch, hidden_size*2)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 3: Direct Self-Attention on embeddings\n",
    "        # ----------------------------\n",
    "        # Add positional embeddings (truncate or expand to the current sequence length)\n",
    "        pos_emb = self.pos_embedding[:, :seq_length, :]  # (1, seq_length, embedding_dim)\n",
    "        self_attn_input = embedded + pos_emb\n",
    "        self_attn_out = self.self_attn_encoder(self_attn_input)  # (batch, seq_length, embedding_dim)\n",
    "        # Global max pooling over the time dimension.\n",
    "        self_attn_features, _ = torch.max(self_attn_out, dim=1)  # (batch, embedding_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Gating: Compute dynamic weights for each branch.\n",
    "        # ----------------------------\n",
    "        gate_cnn = torch.sigmoid(self.gate_cnn(cnn_features))           # (batch, 1)\n",
    "        gate_lstm = torch.sigmoid(self.gate_lstm(lstm_features))          # (batch, 1)\n",
    "        gate_self_attn = torch.sigmoid(self.gate_self_attn(self_attn_features))  # (batch, 1)\n",
    "        \n",
    "        gated_cnn = cnn_features * gate_cnn         # (batch, cnn_out_dim)\n",
    "        gated_lstm = lstm_features * gate_lstm        # (batch, lstm_out_dim)\n",
    "        gated_self_attn = self_attn_features * gate_self_attn  # (batch, self_attn_out_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Feature Fusion and Classification\n",
    "        # ----------------------------\n",
    "        # Concatenate the gated features from all branches.\n",
    "        combined = torch.cat([gated_cnn, gated_lstm, gated_self_attn], dim=1)  # (batch, fused_dim)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.fc(combined)  # (batch, output_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3e43649-a93d-4163-99ae-d11c0640d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6241, Val Loss: 0.4785\n",
      "Epoch 2/10, Train Loss: 0.3864, Val Loss: 0.7370\n",
      "Epoch 3/10, Train Loss: 0.2459, Val Loss: 0.3226\n",
      "Epoch 4/10, Train Loss: 0.1488, Val Loss: 0.1486\n",
      "Epoch 5/10, Train Loss: 0.0780, Val Loss: 0.0310\n",
      "Epoch 6/10, Train Loss: 0.0343, Val Loss: 0.0087\n",
      "Epoch 7/10, Train Loss: 0.0195, Val Loss: 0.0088\n",
      "Epoch 8/10, Train Loss: 0.0124, Val Loss: 0.0055\n",
      "Epoch 9/10, Train Loss: 0.0076, Val Loss: 0.0030\n",
      "Epoch 10/10, Train Loss: 0.0072, Val Loss: 0.0026\n",
      "Training Time on mps: 1153.60 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentUltraUltraAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, lstm_layers=2, transformer_layers=2, num_filters=64, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader,val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ff339-7d8b-44a7-9c0f-c07991d6b38f",
   "metadata": {},
   "source": [
    "# Amazon Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923bb172-137f-4a31-a1d6-a2fd5a6b36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a74a558-23fc-44d2-ba65-7a45bd7a2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess the data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Use the Amazon Polarity dataset from Hugging Face.\n",
    "# We use the provided 'train' split for training and 'test' as our validation set.\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "val_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # Tokenize the text; we use padding and truncation for consistency.\n",
    "    # tokens = tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Remove extra batch dimension from tokens; create a tensor for the label.\n",
    "    return {\"input_ids\": tokens[\"input_ids\"].squeeze(0), \"label\": torch.tensor(examples[\"label\"]).float()}\n",
    "\n",
    "# Apply preprocessing to both training and validation datasets\n",
    "train_tokenized = train_dataset.map(preprocess_data, batched=True)\n",
    "val_tokenized = val_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Convert lists of input_ids and labels into tensors and create TensorDatasets.\n",
    "# (Note: The tokenizer returns lists, so we use torch.stack to combine them.)\n",
    "X_train = torch.stack([torch.tensor(x) for x in train_tokenized[\"input_ids\"]])\n",
    "y_train = torch.tensor(train_tokenized[\"label\"]).float()\n",
    "train_tensor_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "X_val = torch.stack([torch.tensor(x) for x in val_tokenized[\"input_ids\"]])\n",
    "y_val = torch.tensor(val_tokenized[\"label\"]).float()\n",
    "val_tensor_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders for training and validation.\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor_dataset, batch_size=batch_size)\n",
    "\n",
    "# Get vocabulary size from the tokenizer (BERT-base-uncased has vocab size 30522)\n",
    "VOCAB_SIZE = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083b0524-5902-42c2-b9ea-97255cd0b333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define the Sentiment BiLSTM with Attention Model\n",
    "class SentimentBiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(SentimentBiLSTMAttention, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_scores = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        out = self.fc(context_vector)\n",
    "        return out\n",
    "\n",
    "# 3. Implement Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def check(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            return False  # Continue training\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Early stopping triggered after {self.counter} epochs.\")\n",
    "                return True  # Stop training\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d5da20-debe-4116-92cd-d401f9edcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function with Checkpointing\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs):\n",
    "    early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Apply Gradient Clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(outputs)).detach()\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.round(torch.sigmoid(outputs))\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        scheduler.step(val_loss)  # Reduce LR if validation loss stagnates\n",
    "\n",
    "        # Save Best Model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "        # Dynamic Dropout Adjustment\n",
    "        if val_loss > best_val_loss:\n",
    "            model.dropout.p = min(model.dropout.p + 0.05, 0.7)\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if early_stopping.check(val_loss):\n",
    "            break  # Stop training if no improvement\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Load Best Model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53aab302-9f40-4679-9165-cc4c58171665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Improved Hyperparameters\n",
    "embedding_dim = 256  # Remains unchanged\n",
    "hidden_size = 128  # Reduced from 256 to 128\n",
    "num_layers = 2  # Reduced from 3 to 2\n",
    "output_size = 1  # Binary classification\n",
    "dropout = 0.5  # Increased from 0.3 to 0.5 for better regularization\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = SentimentBiLSTMAttention(VOCAB_SIZE, embedding_dim, hidden_size, num_layers, output_size, dropout).to(device)\n",
    "\n",
    "# 4. Define Loss, Optimizer, and Learning Rate Scheduler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63126992-8b7d-4fa5-bc91-bf8a7690489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 0.5541, Train Acc: 0.6879, Val Loss: 0.3979, Val Acc: 0.8190\n",
      "Epoch 2/10: Train Loss: 0.3146, Train Acc: 0.8664, Val Loss: 0.3143, Val Acc: 0.8670\n",
      "Epoch 3/10: Train Loss: 0.2385, Train Acc: 0.9057, Val Loss: 0.3232, Val Acc: 0.8612\n",
      "Epoch 4/10: Train Loss: 0.1773, Train Acc: 0.9350, Val Loss: 0.3446, Val Acc: 0.8698\n",
      "Early stopping triggered after 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "# train_loss, train_acc, val_loss, val_acc = \n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e194ebf0-749e-4e70-8ece-56cd7e126f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Plot both accuracy and loss\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# # Plot accuracy\n",
    "# ax1.plot(range(1, epochs+1), train_acc, label=\"Train Accuracy\")\n",
    "# ax1.plot(range(1, epochs+1), val_acc, label=\"Validation Accuracy\")\n",
    "# ax1.set_xlabel(\"Epoch\")\n",
    "# ax1.set_ylabel(\"Accuracy\")\n",
    "# ax1.set_title(\"Accuracy Over Epochs\")\n",
    "# ax1.legend()\n",
    "# ax1.grid(True)\n",
    "\n",
    "# # Plot loss\n",
    "# ax2.plot(range(1, epochs+1), train_loss, label=\"Train Loss\")\n",
    "# ax2.plot(range(1, epochs+1), val_loss, label=\"Validation Loss\")\n",
    "# ax2.set_xlabel(\"Epoch\")\n",
    "# ax2.set_ylabel(\"Loss\")\n",
    "# ax2.set_title(\"Loss Over Epochs\")\n",
    "# ax2.legend()\n",
    "# ax2.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
