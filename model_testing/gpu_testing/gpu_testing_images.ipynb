{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d737f69-52ad-42a4-8051-43dd01e339b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f876816-ea15-4412-aaad-2cfb07533e88",
   "metadata": {},
   "source": [
    "# CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d6906b-6bba-4d7b-9a86-7cdedf46facf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Define data transformations for training and testing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # Standard normalization for CIFAR-10\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# Split the training set into training and validation subsets (e.g., 90% training, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_subset, val_subset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load CIFAR-10 test data\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93be73ad-6012-4584-9e76-c93b5bbe4672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- Squeeze-and-Excitation Block (same as before) ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        squeeze = self.global_pool(x).view(b, c)\n",
    "        excitation = torch.sigmoid(self.fc2(F.relu(self.fc1(squeeze))))\n",
    "        excitation = excitation.view(b, c, 1, 1)\n",
    "        return x * excitation\n",
    "\n",
    "# --- ResNeXt Block with SE ---\n",
    "class ResNeXtBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cardinality=32, stride=1, downsample=False):\n",
    "        super(ResNeXtBlock, self).__init__()\n",
    "        # Bottleneck channels: typically a reduction factor is applied\n",
    "        mid_channels = out_channels // 2\n",
    "        \n",
    "        # 1x1 convolution for dimension reduction\n",
    "        self.conv_reduce = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)\n",
    "        self.bn_reduce = nn.BatchNorm2d(mid_channels)\n",
    "        \n",
    "        # 3x3 grouped convolution: using the specified cardinality (number of groups)\n",
    "        self.conv_conv = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride,\n",
    "                                   padding=1, groups=cardinality, bias=False)\n",
    "        self.bn_conv = nn.BatchNorm2d(mid_channels)\n",
    "        \n",
    "        # 1x1 convolution for dimension restoration\n",
    "        self.conv_expand = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn_expand = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # SE module to recalibrate features\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        # Shortcut connection in case of dimension mismatch or stride > 1\n",
    "        self.downsample = None\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv_reduce(x)\n",
    "        out = self.bn_reduce(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.conv_conv(out)\n",
    "        out = self.bn_conv(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.conv_expand(out)\n",
    "        out = self.bn_expand(out)\n",
    "        \n",
    "        # Apply SE attention\n",
    "        out = self.se(out)\n",
    "        \n",
    "        # Adjust shortcut if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "# --- Advanced ResNeXt-SE Model for CIFAR-10 ---\n",
    "class ResNeXtSE_CIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10, cardinality=32):\n",
    "        super(ResNeXtSE_CIFAR10, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # Initial convolution: for 32x32 RGB images\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Create layers with increasing feature dimensions.\n",
    "        # You can adjust the number of blocks per layer to balance depth and computation.\n",
    "        self.layer1 = self._make_layer(64, num_blocks=3, cardinality=cardinality, stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks=4, cardinality=cardinality, stride=2)\n",
    "        self.layer3 = self._make_layer(256, num_blocks=6, cardinality=cardinality, stride=2)\n",
    "        self.layer4 = self._make_layer(512, num_blocks=3, cardinality=cardinality, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, out_channels, num_blocks, cardinality, stride):\n",
    "        layers = []\n",
    "        # First block may downsample and increase dimensions\n",
    "        layers.append(ResNeXtBlock(self.in_channels, out_channels, cardinality=cardinality,\n",
    "                                   stride=stride, downsample=True))\n",
    "        self.in_channels = out_channels\n",
    "        # The remaining blocks maintain dimensions\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResNeXtBlock(out_channels, out_channels, cardinality=cardinality, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# --- Early Stopping Implementation ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# --- Training Loop with Early Stopping ---\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, n_epochs=50, patience=10, device='cpu'):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valid_acc = correct / len(valid_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
    "        \n",
    "        # Check early stopping criteria\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e185546-d0fb-4830-893d-323d21550405",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNeXtSE_CIFAR10(num_classes=10, cardinality=32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6658e02d-61c9-4bfb-aa65-234563589bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.5732, Valid Loss: 1.3181, Valid Acc: 0.5210\n",
      "Validation loss decreased (inf --> 1.318115). Saving model...\n",
      "Epoch 2, Train Loss: 1.1814, Valid Loss: 1.1604, Valid Acc: 0.5862\n",
      "Validation loss decreased (1.318115 --> 1.160378). Saving model...\n",
      "Epoch 3, Train Loss: 0.9707, Valid Loss: 0.9100, Valid Acc: 0.6804\n",
      "Validation loss decreased (1.160378 --> 0.910014). Saving model...\n",
      "Epoch 4, Train Loss: 0.7931, Valid Loss: 0.7751, Valid Acc: 0.7196\n",
      "Validation loss decreased (0.910014 --> 0.775052). Saving model...\n",
      "Epoch 5, Train Loss: 0.7093, Valid Loss: 0.7311, Valid Acc: 0.7388\n",
      "Validation loss decreased (0.775052 --> 0.731124). Saving model...\n",
      "Epoch 6, Train Loss: 0.6521, Valid Loss: 0.6698, Valid Acc: 0.7620\n",
      "Validation loss decreased (0.731124 --> 0.669793). Saving model...\n",
      "Epoch 7, Train Loss: 0.5663, Valid Loss: 0.6206, Valid Acc: 0.7830\n",
      "Validation loss decreased (0.669793 --> 0.620552). Saving model...\n",
      "Epoch 8, Train Loss: 0.5330, Valid Loss: 0.5892, Valid Acc: 0.7910\n",
      "Validation loss decreased (0.620552 --> 0.589247). Saving model...\n",
      "Epoch 9, Train Loss: 0.5127, Valid Loss: 0.5813, Valid Acc: 0.7994\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 10, Train Loss: 0.4706, Valid Loss: 0.5599, Valid Acc: 0.8012\n",
      "Validation loss decreased (0.589247 --> 0.559861). Saving model...\n",
      "Epoch 11, Train Loss: 0.4509, Valid Loss: 0.5377, Valid Acc: 0.8120\n",
      "Validation loss decreased (0.559861 --> 0.537738). Saving model...\n",
      "Epoch 12, Train Loss: 0.4427, Valid Loss: 0.5436, Valid Acc: 0.8066\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 13, Train Loss: 0.4232, Valid Loss: 0.5139, Valid Acc: 0.8180\n",
      "Validation loss decreased (0.537738 --> 0.513921). Saving model...\n",
      "Epoch 14, Train Loss: 0.4091, Valid Loss: 0.5271, Valid Acc: 0.8212\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 15, Train Loss: 0.4051, Valid Loss: 0.5309, Valid Acc: 0.8172\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 16, Train Loss: 0.3979, Valid Loss: 0.5093, Valid Acc: 0.8214\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 17, Train Loss: 0.3923, Valid Loss: 0.5165, Valid Acc: 0.8212\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 18, Train Loss: 0.3880, Valid Loss: 0.5060, Valid Acc: 0.8270\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 19, Train Loss: 0.3826, Valid Loss: 0.5097, Valid Acc: 0.8270\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 20, Train Loss: 0.3799, Valid Loss: 0.5019, Valid Acc: 0.8260\n",
      "Validation loss decreased (0.513921 --> 0.501948). Saving model...\n",
      "Epoch 21, Train Loss: 0.3818, Valid Loss: 0.5124, Valid Acc: 0.8252\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 22, Train Loss: 0.3756, Valid Loss: 0.5105, Valid Acc: 0.8206\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 23, Train Loss: 0.3748, Valid Loss: 0.5114, Valid Acc: 0.8230\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 24, Train Loss: 0.3768, Valid Loss: 0.4998, Valid Acc: 0.8270\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 25, Train Loss: 0.3714, Valid Loss: 0.5148, Valid Acc: 0.8238\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 26, Train Loss: 0.3714, Valid Loss: 0.4994, Valid Acc: 0.8322\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 27, Train Loss: 0.3712, Valid Loss: 0.5203, Valid Acc: 0.8182\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 28, Train Loss: 0.3719, Valid Loss: 0.5062, Valid Acc: 0.8302\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 29, Train Loss: 0.3721, Valid Loss: 0.4999, Valid Acc: 0.8320\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 30, Train Loss: 0.3720, Valid Loss: 0.5036, Valid Acc: 0.8192\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, trainloader, valloader, criterion, optimizer, scheduler, n_epochs=50, patience=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468e0a01-613c-4e62-8bde-bf52f022c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/5p2bs93x2xgc8trrh56v3dpc0000gn/T/ipykernel_61814/1084059354.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('checkpoint.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.93%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        # Get predictions by finding the class with the maximum logit\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
