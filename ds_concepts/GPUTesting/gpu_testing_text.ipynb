{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2416e3-36ce-4fc1-a1a3-8ed77f918bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a1025-4947-4ed9-bdb1-4b4ca23eb947",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b933c4-2085-4340-a206-57d17e126eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and preprocess\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "val_dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea368e4-bb16-4c92-8574-7bddaded8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and DataLoader\n",
    "def preprocess_data(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": tokens[\"input_ids\"].squeeze(0), \"label\": torch.tensor(examples[\"label\"]).float()}\n",
    "\n",
    "tokenized_data = dataset.map(preprocess_data, batched=True)\n",
    "X_train = torch.stack([torch.tensor(x) for x in tokenized_data[\"input_ids\"]])\n",
    "y_train = torch.tensor(tokenized_data[\"label\"]).float()\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_tokenized_data = val_dataset.map(preprocess_data, batched=True)\n",
    "X_val = torch.stack([torch.tensor(x) for x in val_tokenized_data[\"input_ids\"]])\n",
    "y_val = torch.tensor(val_tokenized_data[\"label\"]).float()\n",
    "val_dataset = TensorDataset(X_train, y_train)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1dcd1c-888a-43b2-8e6d-3f9ef37cf631",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to train model with validation set\n",
    "def train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=5):\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0  # Accumulate training loss for averaging\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        # Training loop\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "        avg_train_loss = epoch_loss / total_batches  # Compute average training loss\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                y_val_pred = model(x_val)\n",
    "                val_loss += criterion(y_val_pred.squeeze(), y_val).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)  # Compute average validation loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a90d8-d078-4ede-bd69-7ed29fc71be0",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ec349c-07a4-432b-9f97-7f24856145a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model for Sentiment Analysis\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd469193-0f78-422e-8320-c4294a6de9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a5f4d7-1ead-4706-b4cb-3aadbf62ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6953, Val Loss: 0.6932\n",
      "Epoch 2/10, Train Loss: 0.6866, Val Loss: 0.6044\n",
      "Epoch 3/10, Train Loss: 0.4649, Val Loss: 0.3237\n",
      "Epoch 4/10, Train Loss: 0.3120, Val Loss: 0.2239\n",
      "Epoch 5/10, Train Loss: 0.2328, Val Loss: 0.1587\n",
      "Epoch 6/10, Train Loss: 0.1731, Val Loss: 0.1205\n",
      "Epoch 7/10, Train Loss: 0.1291, Val Loss: 0.0935\n",
      "Epoch 8/10, Train Loss: 0.1001, Val Loss: 0.0623\n",
      "Epoch 9/10, Train Loss: 0.0789, Val Loss: 0.0533\n",
      "Epoch 10/10, Train Loss: 0.0687, Val Loss: 0.0479\n",
      "Training Time on cpu: 840.79 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = SentimentLSTM(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3697287-7afb-414a-896b-192ad5527ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6930, Val Loss: 0.6932\n",
      "Epoch 2/10, Train Loss: 0.6692, Val Loss: 0.5740\n",
      "Epoch 3/10, Train Loss: 0.4672, Val Loss: 0.3057\n",
      "Epoch 4/10, Train Loss: 0.2654, Val Loss: 0.1549\n",
      "Epoch 5/10, Train Loss: 0.1578, Val Loss: 0.0797\n",
      "Epoch 6/10, Train Loss: 0.0981, Val Loss: 0.0543\n",
      "Epoch 7/10, Train Loss: 0.0685, Val Loss: 0.0437\n",
      "Epoch 8/10, Train Loss: 0.0581, Val Loss: 0.0384\n",
      "Epoch 9/10, Train Loss: 0.0446, Val Loss: 0.0224\n",
      "Epoch 10/10, Train Loss: 0.0349, Val Loss: 0.0241\n",
      "Training Time on mps: 79.89 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentLSTM(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560875c5-b55f-40ff-9f2d-3ed9d2f76aca",
   "metadata": {},
   "source": [
    "## BiLSTMAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777b3382-9909-4343-887f-196159ee68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentBiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(SentimentBiLSTMAttention, self).__init__()\n",
    "        # Embedding layer converts token indices to embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Bidirectional LSTM to capture context from both directions\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Attention layer: projects each hidden state into a single attention score\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # Final fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings\n",
    "        embedded = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Pass embeddings through the bidirectional LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, sequence_length, hidden_size*2)\n",
    "        \n",
    "        # Compute attention weights for each time step\n",
    "        # The attention layer produces a score for each hidden state\n",
    "        attn_scores = torch.tanh(self.attention(lstm_out))  # (batch_size, sequence_length, 1)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)          # (batch_size, sequence_length, 1)\n",
    "        \n",
    "        # Compute the context vector as the weighted sum of LSTM outputs\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)  # (batch_size, hidden_size*2)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Final classification output\n",
    "        out = self.fc(context_vector)  # (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e4e61a-4bbe-42e2-aa7d-4e6f4b70dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5532, Val Loss: 0.3891\n",
      "Epoch 2/10, Train Loss: 0.3604, Val Loss: 0.2694\n",
      "Epoch 3/10, Train Loss: 0.2677, Val Loss: 0.2169\n",
      "Epoch 4/10, Train Loss: 0.2121, Val Loss: 0.1533\n",
      "Epoch 5/10, Train Loss: 0.1650, Val Loss: 0.1140\n",
      "Epoch 6/10, Train Loss: 0.1109, Val Loss: 0.0693\n",
      "Epoch 7/10, Train Loss: 0.0731, Val Loss: 0.0407\n",
      "Epoch 8/10, Train Loss: 0.0449, Val Loss: 0.0339\n",
      "Epoch 9/10, Train Loss: 0.0584, Val Loss: 0.0358\n",
      "Epoch 10/10, Train Loss: 0.0206, Val Loss: 0.0078\n",
      "Training Time on mps: 178.21 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentBiLSTMAttention(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a001ae4-1047-440b-b891-a8aefb3b6bd8",
   "metadata": {},
   "source": [
    "## SentimentAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13ef8f77-9af8-4031-8baa-a31ff1a85730",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        output_size, \n",
    "        dropout=0.5, \n",
    "        n_heads=4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Number of tokens in the vocabulary.\n",
    "            embedding_dim (int): Dimensionality of the word embeddings.\n",
    "            hidden_size (int): Hidden state size of the LSTM.\n",
    "            num_layers (int): Number of LSTM layers.\n",
    "            output_size (int): Number of output classes (e.g., sentiment labels).\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of attention heads for multi-head attention.\n",
    "        \"\"\"\n",
    "        super(SentimentAdvanced, self).__init__()\n",
    "        # Embedding layer converts token indices to embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Bidirectional LSTM to capture context from both directions.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Multi-head self-attention layer.\n",
    "        # Note: embed_dim for attention is hidden_size * 2 due to bidirectionality.\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2, \n",
    "            num_heads=n_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for stabilizing training.\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        \n",
    "        # Final fully connected layer for classification.\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        \n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, output_size) containing class scores.\n",
    "        \"\"\"\n",
    "        # 1. Embed the input tokens.\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # 2. Pass the embeddings through the bidirectional LSTM.\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # 3. Apply multi-head self-attention.\n",
    "        #    Using LSTM outputs as query, key, and value.\n",
    "        attn_out, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)\n",
    "        # 4. Add a residual connection and normalize.\n",
    "        attn_out = self.layer_norm(attn_out + lstm_out)\n",
    "        \n",
    "        # 5. Pool the sequence output (max pooling over the time dimension).\n",
    "        #    This extracts the most salient features across the sequence.\n",
    "        pooled, _ = torch.max(attn_out, dim=1)  # Shape: (batch_size, hidden_size*2)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        # 6. Final classification layer.\n",
    "        out = self.fc(pooled)  # Shape: (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acdbd921-99b4-40d4-8dd6-d7f4623dcde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6415, Val Loss: 0.4701\n",
      "Epoch 2/10, Train Loss: 0.3907, Val Loss: 0.2871\n",
      "Epoch 3/10, Train Loss: 0.2813, Val Loss: 0.2021\n",
      "Epoch 4/10, Train Loss: 0.2164, Val Loss: 0.1586\n",
      "Epoch 5/10, Train Loss: 0.1595, Val Loss: 0.1831\n",
      "Epoch 6/10, Train Loss: 0.1035, Val Loss: 0.0655\n",
      "Epoch 7/10, Train Loss: 0.0766, Val Loss: 0.0398\n",
      "Epoch 8/10, Train Loss: 0.0447, Val Loss: 0.0215\n",
      "Epoch 9/10, Train Loss: 0.0303, Val Loss: 0.0166\n",
      "Epoch 10/10, Train Loss: 0.0258, Val Loss: 0.0163\n",
      "Training Time on mps: 285.63 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, num_layers=2, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader,val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c2ddd-884a-4460-b819-1204f2d92fd1",
   "metadata": {},
   "source": [
    "## SentimentUltraAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c8597a-2559-4d5b-8cfa-a2946a10e9bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentUltraAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        lstm_layers,\n",
    "        transformer_layers,\n",
    "        num_filters,\n",
    "        output_size,\n",
    "        dropout=0.5,\n",
    "        n_heads=4,\n",
    "        cnn_kernel_sizes=[2, 3, 4]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of word embeddings.\n",
    "            hidden_size (int): Hidden state size for the LSTM.\n",
    "            lstm_layers (int): Number of LSTM layers.\n",
    "            transformer_layers (int): Number of Transformer encoder layers.\n",
    "            num_filters (int): Number of CNN filters per kernel size.\n",
    "            output_size (int): Number of output classes (e.g., sentiment categories).\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of heads in the Transformer encoder.\n",
    "            cnn_kernel_sizes (list): List of kernel sizes for the CNN branch.\n",
    "        \"\"\"\n",
    "        super(SentimentUltraAdvanced, self).__init__()\n",
    "        # Embedding layer: converts token indices to embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # CNN branch: a set of 1D convolutional layers with different kernel sizes.\n",
    "        self.cnn_convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=ks\n",
    "            )\n",
    "            for ks in cnn_kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # LSTM branch: Bidirectional LSTM to capture sequential context.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder: further refines LSTM outputs using multi-head self-attention.\n",
    "        # (Note: With PyTorch 1.9+ you can set batch_first=True in TransformerEncoderLayer.)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size * 2,  # Because LSTM is bidirectional.\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "        \n",
    "        # Compute the combined feature dimension.\n",
    "        # CNN branch: len(cnn_kernel_sizes) * num_filters.\n",
    "        # LSTM/Transformer branch: hidden_size * 2.\n",
    "        combined_dim = num_filters * len(cnn_kernel_sizes) + hidden_size * 2\n",
    "        \n",
    "        # Final fully connected layer for classification.\n",
    "        self.fc = nn.Linear(combined_dim, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # 2. CNN branch:\n",
    "        # Permute to (batch_size, embedding_dim, seq_length) for Conv1d.\n",
    "        cnn_input = embedded.permute(0, 2, 1)\n",
    "        cnn_features = []\n",
    "        for conv in self.cnn_convs:\n",
    "            # Apply convolution -> non-linearity -> global max pooling.\n",
    "            conv_out = conv(cnn_input)         # Shape: (batch, num_filters, L_out)\n",
    "            conv_out = F.relu(conv_out)\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.shape[2])  # Shape: (batch, num_filters, 1)\n",
    "            pooled = pooled.squeeze(2)         # Shape: (batch, num_filters)\n",
    "            cnn_features.append(pooled)\n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # Shape: (batch, num_filters * len(cnn_kernel_sizes))\n",
    "        \n",
    "        # 3. LSTM + Transformer branch:\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch, seq_length, hidden_size*2)\n",
    "        # Refine LSTM outputs with Transformer encoder.\n",
    "        transformer_out = self.transformer_encoder(lstm_out)  # Shape: (batch, seq_length, hidden_size*2)\n",
    "        # Global average pooling over time (sequence length) dimension.\n",
    "        transformer_features = torch.mean(transformer_out, dim=1)  # Shape: (batch, hidden_size*2)\n",
    "        \n",
    "        # 4. Feature Fusion:\n",
    "        combined = torch.cat([cnn_features, transformer_features], dim=1)  # Shape: (batch, combined_dim)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # 5. Final classification layer.\n",
    "        output = self.fc(combined)  # Shape: (batch, output_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae9a3d99-f69c-4925-a46f-a1d1ad1e8e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6708, Val Loss: 0.5174\n",
      "Epoch 2/10, Train Loss: 0.5323, Val Loss: 0.4471\n",
      "Epoch 3/10, Train Loss: 0.4708, Val Loss: 0.3932\n",
      "Epoch 4/10, Train Loss: 0.4366, Val Loss: 0.3298\n",
      "Epoch 5/10, Train Loss: 0.3939, Val Loss: 0.2932\n",
      "Epoch 6/10, Train Loss: 0.3687, Val Loss: 0.2793\n",
      "Epoch 7/10, Train Loss: 0.3341, Val Loss: 0.2353\n",
      "Epoch 8/10, Train Loss: 0.3080, Val Loss: 0.2031\n",
      "Epoch 9/10, Train Loss: 0.2797, Val Loss: 0.1755\n",
      "Epoch 10/10, Train Loss: 0.2489, Val Loss: 0.1490\n",
      "Training Time on mps: 769.11 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentUltraAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, lstm_layers=2, transformer_layers=2, num_filters=64, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader, val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b0bda-8356-49d5-a417-0f86782dca91",
   "metadata": {},
   "source": [
    "## SentimentUltraUltraAdvanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a58dbc4e-c8b0-4a60-8277-9dd8cba9a090",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentUltraUltraAdvanced(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        lstm_layers,\n",
    "        transformer_layers,\n",
    "        num_filters,\n",
    "        output_size,\n",
    "        dropout=0.5,\n",
    "        n_heads=4,\n",
    "        cnn_kernel_sizes=[2, 3, 4],\n",
    "        self_attn_layers=2,\n",
    "        max_seq_length=512  # adjust based on your expected maximum sequence length\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Vocabulary size.\n",
    "            embedding_dim (int): Dimension of word embeddings.\n",
    "            hidden_size (int): Hidden size for the LSTM.\n",
    "            lstm_layers (int): Number of LSTM layers.\n",
    "            transformer_layers (int): Number of Transformer encoder layers for the LSTM branch.\n",
    "            num_filters (int): Number of CNN filters per kernel size.\n",
    "            output_size (int): Number of output classes.\n",
    "            dropout (float): Dropout probability.\n",
    "            n_heads (int): Number of attention heads in Transformer encoders.\n",
    "            cnn_kernel_sizes (list): List of kernel sizes for the CNN branch.\n",
    "            self_attn_layers (int): Number of Transformer encoder layers in the self-attention branch.\n",
    "            max_seq_length (int): Maximum expected sequence length (for positional embeddings).\n",
    "        \"\"\"\n",
    "        super(SentimentUltraUltraAdvanced, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Learnable positional embeddings for the self-attention branch\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, embedding_dim))\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 1: CNN for local features\n",
    "        # ----------------------------\n",
    "        # Create one convolution per kernel size.\n",
    "        self.cnn_convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            ) for k in cnn_kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 2: LSTM + Transformer for sequential modeling\n",
    "        # ----------------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        # Transformer encoder refines the LSTM outputs.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size * 2,  # bidirectional\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm_transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 3: Direct Self-Attention on embeddings\n",
    "        # ----------------------------\n",
    "        encoder_layer2 = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.self_attn_encoder = nn.TransformerEncoder(encoder_layer2, num_layers=self_attn_layers)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Gating Mechanism to fuse branch features dynamically\n",
    "        # ----------------------------\n",
    "        # Compute dimensions for each branch's output:\n",
    "        cnn_out_dim = num_filters * len(cnn_kernel_sizes)\n",
    "        lstm_out_dim = hidden_size * 2  # from bidirectional LSTM\n",
    "        self_attn_out_dim = embedding_dim\n",
    "        \n",
    "        # Linear layers to compute a gate (a scalar weight) for each branch.\n",
    "        self.gate_cnn = nn.Linear(cnn_out_dim, 1)\n",
    "        self.gate_lstm = nn.Linear(lstm_out_dim, 1)\n",
    "        self.gate_self_attn = nn.Linear(self_attn_out_dim, 1)\n",
    "        \n",
    "        # Final fusion dimension is the concatenation of all branch outputs.\n",
    "        fused_dim = cnn_out_dim + lstm_out_dim + self_attn_out_dim\n",
    "        \n",
    "        # Final classification layer.\n",
    "        self.fc = nn.Linear(fused_dim, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, sequence_length) with token indices.\n",
    "        Returns:\n",
    "            Tensor: Logits of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.size()\n",
    "        # 1. Embedding lookup.\n",
    "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 1: CNN\n",
    "        # ----------------------------\n",
    "        # For Conv1d, we need shape: (batch_size, embedding_dim, seq_length)\n",
    "        cnn_input = embedded.permute(0, 2, 1)\n",
    "        cnn_features = []\n",
    "        for conv in self.cnn_convs:\n",
    "            conv_out = F.relu(conv(cnn_input))  # (batch, num_filters, L_out)\n",
    "            # Global max pooling over the temporal (L_out) dimension.\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.size(2)).squeeze(2)  # (batch, num_filters)\n",
    "            cnn_features.append(pooled)\n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # (batch, num_filters * len(cnn_kernel_sizes))\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 2: LSTM + Transformer\n",
    "        # ----------------------------\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch, seq_length, hidden_size*2)\n",
    "        # Refine with Transformer encoder.\n",
    "        lstm_transformed = self.lstm_transformer(lstm_out)  # (batch, seq_length, hidden_size*2)\n",
    "        # Global average pooling over the time dimension.\n",
    "        lstm_features = torch.mean(lstm_transformed, dim=1)  # (batch, hidden_size*2)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Branch 3: Direct Self-Attention on embeddings\n",
    "        # ----------------------------\n",
    "        # Add positional embeddings (truncate or expand to the current sequence length)\n",
    "        pos_emb = self.pos_embedding[:, :seq_length, :]  # (1, seq_length, embedding_dim)\n",
    "        self_attn_input = embedded + pos_emb\n",
    "        self_attn_out = self.self_attn_encoder(self_attn_input)  # (batch, seq_length, embedding_dim)\n",
    "        # Global max pooling over the time dimension.\n",
    "        self_attn_features, _ = torch.max(self_attn_out, dim=1)  # (batch, embedding_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Gating: Compute dynamic weights for each branch.\n",
    "        # ----------------------------\n",
    "        gate_cnn = torch.sigmoid(self.gate_cnn(cnn_features))           # (batch, 1)\n",
    "        gate_lstm = torch.sigmoid(self.gate_lstm(lstm_features))          # (batch, 1)\n",
    "        gate_self_attn = torch.sigmoid(self.gate_self_attn(self_attn_features))  # (batch, 1)\n",
    "        \n",
    "        gated_cnn = cnn_features * gate_cnn         # (batch, cnn_out_dim)\n",
    "        gated_lstm = lstm_features * gate_lstm        # (batch, lstm_out_dim)\n",
    "        gated_self_attn = self_attn_features * gate_self_attn  # (batch, self_attn_out_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Feature Fusion and Classification\n",
    "        # ----------------------------\n",
    "        # Concatenate the gated features from all branches.\n",
    "        combined = torch.cat([gated_cnn, gated_lstm, gated_self_attn], dim=1)  # (batch, fused_dim)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.fc(combined)  # (batch, output_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3e43649-a93d-4163-99ae-d11c0640d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5843, Val Loss: 0.5486\n",
      "Epoch 2/10, Train Loss: 0.3557, Val Loss: 0.2226\n",
      "Epoch 3/10, Train Loss: 0.2196, Val Loss: 0.1417\n",
      "Epoch 4/10, Train Loss: 0.1160, Val Loss: 0.0935\n",
      "Epoch 5/10, Train Loss: 0.0654, Val Loss: 0.7176\n",
      "Epoch 6/10, Train Loss: 0.0422, Val Loss: 0.1328\n",
      "Epoch 7/10, Train Loss: 0.0185, Val Loss: 0.0091\n",
      "Epoch 8/10, Train Loss: 0.0080, Val Loss: 0.0029\n",
      "Epoch 9/10, Train Loss: 0.0046, Val Loss: 0.0047\n",
      "Epoch 10/10, Train Loss: 0.0030, Val Loss: 0.0014\n",
      "Training Time on mps: 1098.53 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SentimentUltraUltraAdvanced(VOCAB_SIZE, embedding_dim=100, hidden_size=128, lstm_layers=2, transformer_layers=2, num_filters=64, output_size=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Best choice\n",
    "time_taken = train_model(device, train_loader,val_loader, model, criterion, optimizer, epochs=10)\n",
    "print(f\"Training Time on {device}: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb48597-3938-4b94-9d36-f2accc40e5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e50fe6a-7089-4bb2-b627-787dc07ad398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())  # Shows number of CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbc338-8edf-4259-8082-83d31e1d044c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
