{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning from: https://realpython.com/sentiment-analysis-python/. I will be using that website to make notes and follow along with their tutorial to better understand the concepts in Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n",
    "\n",
    "The process of breaking down chunks of text into smaller pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") #load spaCy’s English model\n",
    "doc = nlp(text) #tokenize the text by passing it into the nlp constructor\n",
    "token_list = [token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words\n",
    "\n",
    "Stop words are words that are only useful in human language but not in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Words\n",
    "It entails condensing all forms of a word into a single representation of that word. For example, removing conjugation such as “watched,” “watching,” and “watches” can all be normalized into “watch.”\n",
    "\n",
    "2 ways of doing it:\n",
    "1. Stemming\n",
    "2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With stemming, a word is cut off at its stem, the smallest unit of that word from which you can create the descendant words. Stemming will miss the relationship between “feel” and “felt”. \n",
    "\n",
    "Lemmatization seeks to address this issue. This process uses a data structure that relates all forms of a word back to its simplest form, or lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [f\"Token: {token}, lemma: {token.lemma_}\" for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Token: \\n, lemma: \\n',\n",
       " 'Token: Dave, lemma: Dave',\n",
       " 'Token: watched, lemma: watch',\n",
       " 'Token: forest, lemma: forest',\n",
       " 'Token: burned, lemma: burn',\n",
       " 'Token: hill, lemma: hill',\n",
       " 'Token: ,, lemma: ,',\n",
       " 'Token: \\n, lemma: \\n',\n",
       " 'Token: miles, lemma: mile',\n",
       " 'Token: house, lemma: house',\n",
       " 'Token: ., lemma: .',\n",
       " 'Token: car, lemma: car',\n",
       " 'Token: \\n, lemma: \\n',\n",
       " 'Token: hastily, lemma: hastily',\n",
       " 'Token: packed, lemma: pack',\n",
       " 'Token: Marta, lemma: Marta',\n",
       " 'Token: inside, lemma: inside',\n",
       " 'Token: trying, lemma: try',\n",
       " 'Token: round, lemma: round',\n",
       " 'Token: \\n, lemma: \\n',\n",
       " 'Token: pets, lemma: pet',\n",
       " 'Token: ., lemma: .',\n",
       " 'Token: \", lemma: \"',\n",
       " 'Token: ?, lemma: ?',\n",
       " 'Token: \", lemma: \"',\n",
       " 'Token: wondered, lemma: wonder',\n",
       " 'Token: \\n, lemma: \\n',\n",
       " 'Token: continued, lemma: continue',\n",
       " 'Token: wait, lemma: wait',\n",
       " 'Token: Marta, lemma: Marta',\n",
       " 'Token: appear, lemma: appear',\n",
       " 'Token: pets, lemma: pet',\n",
       " 'Token: ., lemma: .',\n",
       " 'Token: \\n, lemma: \\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Text\n",
    "\n",
    "Vectorization is a process that transforms a token into a vector, or a numeric array that, in the context of NLP, is unique to and represents various features of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8371646 ,  1.4529226 , -1.6147211 ,  0.678362  , -0.6594443 ,\n",
       "        1.6417935 ,  0.5796405 ,  2.3021278 , -0.13260496,  0.5750932 ,\n",
       "        1.5654886 , -0.6938864 , -0.59607106, -1.5377437 ,  1.9425622 ,\n",
       "       -2.4552505 ,  1.2321601 ,  1.0434952 , -1.5102385 , -0.5787632 ,\n",
       "        0.12055647,  3.6501784 ,  2.6160972 , -0.5710199 , -1.5221789 ,\n",
       "        0.00629176,  0.22760668, -1.922073  , -1.6252862 , -4.226225  ,\n",
       "       -3.495663  , -3.312053  ,  0.81387717, -0.00677544, -0.11603224,\n",
       "        1.4620426 ,  3.0751472 ,  0.35958546, -0.22527039, -2.743926  ,\n",
       "        1.269633  ,  4.606786  ,  0.34034157, -2.1272311 ,  1.2619178 ,\n",
       "       -4.209798  ,  5.452852  ,  1.6940253 , -2.5972986 ,  0.95049495,\n",
       "       -1.910578  , -2.374927  , -1.4227567 , -2.2528825 , -1.799806  ,\n",
       "        1.607501  ,  2.9914255 ,  2.8065152 , -1.2510269 , -0.54964066,\n",
       "       -0.49980402, -1.3882618 , -0.470479  , -2.9670253 ,  1.7884955 ,\n",
       "        4.5282774 , -1.2602427 , -0.14885521,  1.0419178 , -0.08892632,\n",
       "       -1.138275  ,  2.242618  ,  1.5077229 , -1.5030195 ,  2.528098  ,\n",
       "       -1.6761329 ,  0.16694719,  2.123961  ,  0.02546412,  0.38754445,\n",
       "        0.8911977 , -0.07678384, -2.0690763 , -1.1211847 ,  1.4821006 ,\n",
       "        1.1989193 ,  2.1933236 ,  0.5296372 ,  3.0646474 , -1.7223308 ,\n",
       "       -1.3634219 , -0.47471118, -1.7648507 ,  3.565178  , -2.394205  ,\n",
       "       -1.3800384 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1].vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
